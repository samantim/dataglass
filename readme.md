# üß† dataglass

**A modular and lightweight library for preprocessing, analysis, and modeling structured datasets in Python.**

`dataglass` provides an easy-to-use yet powerful framework to handle essential preprocessing tasks such as missing value handling, duplicate removal, outlier detection and management, feature encoding, type conversion, and feature scaling ‚Äî all designed to integrate with custom pipeline workflows.

---

## üöÄ Preprocessing Features

- **Missing Value Imputation** (Drop, Imputation by Datatype: Mean/Median/Mode, Imputation by Adjacent Values: Forward/Backward Fill, Interpolation: Linear/Time-based)
- **Duplicate Detection & Removal** (Exact & Fuzzy Matching with Full and Partial Similarity Check)
- **Outlier Detection & Handling** (Detection: IQR/Z-Score/Isolation Forest/LOF, Handling: Drop/Replace with Median/Cap with Boundaries, Visualization: Before vs After Boxplot/Histogram)
- **Feature encoding** (Label Encoding/Onehot Encoding/Hashing)
- **Type Conversion** (Automatic Datatype Inferring/User-defined Type Conversion)
- **Feature Scaling** (Scaling: Min-Max/Z-Score/Robust Scaling, Normalization: L2 Normalization)
- **Pipeline Compatibility** (Custom pipeline interface for reusable workflows)

---

## üì¶ Installation

```bash
pip install dataglass
```
Requires: pandas, numpy, scikit-learn, seaborn, matplotlib, rapidfuzz, category_encoders

---

## üìò Usage Examples

### ‚ùì Missing Value Handling
```bash
import dataglass as dg
import pandas as pd
import numpy as np

df = pd.DataFrame({
    "name": ["John", "Jane", "Jack"],
    "age": [40, np.nan, 50],
    "gender": ["male", "female", "male"]
})

handle_missing = dg.HandleMissingStep(dg.HandleMissingMethod.DROP)
handle_duplicate = dg.HandleDuplicateStep(dg.HandleDuplicateMethod.EXACT)
handle_outlier = dg.HandleOutlierStep(dg.DetectOutlierMethod.IQR, dg.HandleOutlierMethod.DROP)
encode_feature = dg.EncodeFeatureStep(dg.FeatureEncodingMethod.LABEL_ENCODING, ["gender"])
scale_feature = dg.ScaleFeatureStep({"column": ["age"], "scaling_method": ["MINMAX_SCALING"]})
type_conversion = dg.TypeConversionStep(dg.ConvertDatatypeMethod.AUTO, verbose=True)

dp = dg.DataPipeline([
    handle_missing,
    handle_duplicate,
    type_conversion,
    handle_outlier,
    scale_feature,
    encode_feature,
])

df_cleaned = dp.apply(df)
print(df_cleaned)
```
